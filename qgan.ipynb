{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2649,
     "status": "ok",
     "timestamp": 1664680795405,
     "user": {
      "displayName": "蕭仁鴻",
      "userId": "08849482633546520155"
     },
     "user_tz": -480
    },
    "id": "QiW3iUIZd3gp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import pennylane as qml\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "executionInfo": {
     "elapsed": 683,
     "status": "ok",
     "timestamp": 1664680815154,
     "user": {
      "displayName": "蕭仁鴻",
      "userId": "08849482633546520155"
     },
     "user_tz": -480
    },
    "id": "XWY3zehseWY_",
    "outputId": "b441dcb6-96eb-40f8-de6c-656345749dd3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>pct_change</th>\n",
       "      <th>log_change</th>\n",
       "      <th>7ma</th>\n",
       "      <th>14ma</th>\n",
       "      <th>...</th>\n",
       "      <th>investment_dif</th>\n",
       "      <th>foreign_buy</th>\n",
       "      <th>foreign_sell</th>\n",
       "      <th>foreign_dif</th>\n",
       "      <th>FT_3components</th>\n",
       "      <th>FT_6components</th>\n",
       "      <th>FT_9components</th>\n",
       "      <th>FT_27components</th>\n",
       "      <th>FT_81components</th>\n",
       "      <th>FT_100components</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010/2/10</th>\n",
       "      <td>52.250000</td>\n",
       "      <td>51.700001</td>\n",
       "      <td>51.950001</td>\n",
       "      <td>52.099998</td>\n",
       "      <td>11219</td>\n",
       "      <td>52.099998</td>\n",
       "      <td>0.010669</td>\n",
       "      <td>0.010613</td>\n",
       "      <td>51.943241</td>\n",
       "      <td>52.708595</td>\n",
       "      <td>...</td>\n",
       "      <td>1454164689</td>\n",
       "      <td>1.551380e+10</td>\n",
       "      <td>2.549564e+10</td>\n",
       "      <td>-9981838210</td>\n",
       "      <td>96.358920</td>\n",
       "      <td>98.341083</td>\n",
       "      <td>94.157682</td>\n",
       "      <td>97.504515</td>\n",
       "      <td>95.290389</td>\n",
       "      <td>94.816094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010/2/22</th>\n",
       "      <td>53.150002</td>\n",
       "      <td>52.599998</td>\n",
       "      <td>53.049999</td>\n",
       "      <td>52.599998</td>\n",
       "      <td>11187</td>\n",
       "      <td>52.599998</td>\n",
       "      <td>0.009597</td>\n",
       "      <td>0.009551</td>\n",
       "      <td>52.107470</td>\n",
       "      <td>52.693884</td>\n",
       "      <td>...</td>\n",
       "      <td>771651699</td>\n",
       "      <td>2.846749e+10</td>\n",
       "      <td>2.503619e+10</td>\n",
       "      <td>3431302085</td>\n",
       "      <td>96.242195</td>\n",
       "      <td>98.002049</td>\n",
       "      <td>93.597325</td>\n",
       "      <td>95.929373</td>\n",
       "      <td>90.414471</td>\n",
       "      <td>88.712660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010/2/23</th>\n",
       "      <td>52.750000</td>\n",
       "      <td>52.200001</td>\n",
       "      <td>52.599998</td>\n",
       "      <td>52.599998</td>\n",
       "      <td>9571</td>\n",
       "      <td>52.599998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52.230624</td>\n",
       "      <td>52.681192</td>\n",
       "      <td>...</td>\n",
       "      <td>-387699136</td>\n",
       "      <td>2.422218e+10</td>\n",
       "      <td>2.360860e+10</td>\n",
       "      <td>613580581</td>\n",
       "      <td>96.125140</td>\n",
       "      <td>97.662731</td>\n",
       "      <td>93.037830</td>\n",
       "      <td>94.352361</td>\n",
       "      <td>85.604138</td>\n",
       "      <td>82.741271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010/2/24</th>\n",
       "      <td>52.549999</td>\n",
       "      <td>52.150002</td>\n",
       "      <td>52.150002</td>\n",
       "      <td>52.250000</td>\n",
       "      <td>11026</td>\n",
       "      <td>52.250000</td>\n",
       "      <td>-0.006654</td>\n",
       "      <td>-0.006676</td>\n",
       "      <td>52.235469</td>\n",
       "      <td>52.623011</td>\n",
       "      <td>...</td>\n",
       "      <td>483080557</td>\n",
       "      <td>2.027639e+10</td>\n",
       "      <td>2.546846e+10</td>\n",
       "      <td>-5192069643</td>\n",
       "      <td>96.007757</td>\n",
       "      <td>97.323153</td>\n",
       "      <td>92.479275</td>\n",
       "      <td>92.775246</td>\n",
       "      <td>80.907607</td>\n",
       "      <td>76.994296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010/2/25</th>\n",
       "      <td>52.299999</td>\n",
       "      <td>51.349998</td>\n",
       "      <td>52.250000</td>\n",
       "      <td>51.650002</td>\n",
       "      <td>20361</td>\n",
       "      <td>51.650002</td>\n",
       "      <td>-0.011483</td>\n",
       "      <td>-0.011550</td>\n",
       "      <td>52.089087</td>\n",
       "      <td>52.491931</td>\n",
       "      <td>...</td>\n",
       "      <td>-484640740</td>\n",
       "      <td>2.259238e+10</td>\n",
       "      <td>3.143820e+10</td>\n",
       "      <td>-8845826819</td>\n",
       "      <td>95.890048</td>\n",
       "      <td>96.983341</td>\n",
       "      <td>91.921743</td>\n",
       "      <td>91.199799</td>\n",
       "      <td>76.371194</td>\n",
       "      <td>71.557895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020/12/25</th>\n",
       "      <td>119.599998</td>\n",
       "      <td>118.599998</td>\n",
       "      <td>119.099998</td>\n",
       "      <td>118.949997</td>\n",
       "      <td>2801317</td>\n",
       "      <td>118.949997</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>118.587579</td>\n",
       "      <td>118.090677</td>\n",
       "      <td>...</td>\n",
       "      <td>373419980</td>\n",
       "      <td>2.930742e+10</td>\n",
       "      <td>2.403325e+10</td>\n",
       "      <td>5274170949</td>\n",
       "      <td>111.049753</td>\n",
       "      <td>127.320992</td>\n",
       "      <td>121.886350</td>\n",
       "      <td>124.377775</td>\n",
       "      <td>120.608542</td>\n",
       "      <td>121.100219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020/12/28</th>\n",
       "      <td>120.000000</td>\n",
       "      <td>119.050003</td>\n",
       "      <td>119.050003</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>3407442</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.008827</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>118.940684</td>\n",
       "      <td>118.345254</td>\n",
       "      <td>...</td>\n",
       "      <td>139024268</td>\n",
       "      <td>5.145881e+10</td>\n",
       "      <td>3.850894e+10</td>\n",
       "      <td>12949871214</td>\n",
       "      <td>111.054398</td>\n",
       "      <td>127.489588</td>\n",
       "      <td>122.247031</td>\n",
       "      <td>124.754413</td>\n",
       "      <td>121.300176</td>\n",
       "      <td>121.903124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020/12/29</th>\n",
       "      <td>120.199997</td>\n",
       "      <td>119.349998</td>\n",
       "      <td>120.050003</td>\n",
       "      <td>119.900002</td>\n",
       "      <td>3093979</td>\n",
       "      <td>119.900002</td>\n",
       "      <td>-0.000833</td>\n",
       "      <td>-0.000834</td>\n",
       "      <td>119.180513</td>\n",
       "      <td>118.552554</td>\n",
       "      <td>...</td>\n",
       "      <td>-767399344</td>\n",
       "      <td>4.903891e+10</td>\n",
       "      <td>5.029541e+10</td>\n",
       "      <td>-1256508512</td>\n",
       "      <td>111.058523</td>\n",
       "      <td>127.656402</td>\n",
       "      <td>122.607526</td>\n",
       "      <td>125.124296</td>\n",
       "      <td>122.062133</td>\n",
       "      <td>122.757176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020/12/30</th>\n",
       "      <td>121.599998</td>\n",
       "      <td>119.900002</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>121.599998</td>\n",
       "      <td>5368204</td>\n",
       "      <td>121.599998</td>\n",
       "      <td>0.014178</td>\n",
       "      <td>0.014079</td>\n",
       "      <td>119.785385</td>\n",
       "      <td>118.958879</td>\n",
       "      <td>...</td>\n",
       "      <td>-683459786</td>\n",
       "      <td>7.361430e+10</td>\n",
       "      <td>4.678062e+10</td>\n",
       "      <td>26833686455</td>\n",
       "      <td>111.062129</td>\n",
       "      <td>127.821414</td>\n",
       "      <td>122.967770</td>\n",
       "      <td>125.487274</td>\n",
       "      <td>122.888955</td>\n",
       "      <td>123.652424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020/12/31</th>\n",
       "      <td>122.400002</td>\n",
       "      <td>121.699997</td>\n",
       "      <td>122.150002</td>\n",
       "      <td>122.250000</td>\n",
       "      <td>5741338</td>\n",
       "      <td>122.250000</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>120.401539</td>\n",
       "      <td>119.397695</td>\n",
       "      <td>...</td>\n",
       "      <td>366605219</td>\n",
       "      <td>5.869081e+10</td>\n",
       "      <td>5.299289e+10</td>\n",
       "      <td>5697920877</td>\n",
       "      <td>111.065213</td>\n",
       "      <td>127.984605</td>\n",
       "      <td>123.327696</td>\n",
       "      <td>125.843220</td>\n",
       "      <td>123.773486</td>\n",
       "      <td>124.578062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2665 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  High         Low        Open       Close   Volume  \\\n",
       "Date                                                                  \n",
       "2010/2/10    52.250000   51.700001   51.950001   52.099998    11219   \n",
       "2010/2/22    53.150002   52.599998   53.049999   52.599998    11187   \n",
       "2010/2/23    52.750000   52.200001   52.599998   52.599998     9571   \n",
       "2010/2/24    52.549999   52.150002   52.150002   52.250000    11026   \n",
       "2010/2/25    52.299999   51.349998   52.250000   51.650002    20361   \n",
       "...                ...         ...         ...         ...      ...   \n",
       "2020/12/25  119.599998  118.599998  119.099998  118.949997  2801317   \n",
       "2020/12/28  120.000000  119.050003  119.050003  120.000000  3407442   \n",
       "2020/12/29  120.199997  119.349998  120.050003  119.900002  3093979   \n",
       "2020/12/30  121.599998  119.900002  120.000000  121.599998  5368204   \n",
       "2020/12/31  122.400002  121.699997  122.150002  122.250000  5741338   \n",
       "\n",
       "             Adj Close  pct_change  log_change         7ma        14ma  ...  \\\n",
       "Date                                                                    ...   \n",
       "2010/2/10    52.099998    0.010669    0.010613   51.943241   52.708595  ...   \n",
       "2010/2/22    52.599998    0.009597    0.009551   52.107470   52.693884  ...   \n",
       "2010/2/23    52.599998    0.000000    0.000000   52.230624   52.681192  ...   \n",
       "2010/2/24    52.250000   -0.006654   -0.006676   52.235469   52.623011  ...   \n",
       "2010/2/25    51.650002   -0.011483   -0.011550   52.089087   52.491931  ...   \n",
       "...                ...         ...         ...         ...         ...  ...   \n",
       "2020/12/25  118.949997    0.001263    0.001262  118.587579  118.090677  ...   \n",
       "2020/12/28  120.000000    0.008827    0.008789  118.940684  118.345254  ...   \n",
       "2020/12/29  119.900002   -0.000833   -0.000834  119.180513  118.552554  ...   \n",
       "2020/12/30  121.599998    0.014178    0.014079  119.785385  118.958879  ...   \n",
       "2020/12/31  122.250000    0.005345    0.005331  120.401539  119.397695  ...   \n",
       "\n",
       "            investment_dif   foreign_buy  foreign_sell  foreign_dif  \\\n",
       "Date                                                                  \n",
       "2010/2/10       1454164689  1.551380e+10  2.549564e+10  -9981838210   \n",
       "2010/2/22        771651699  2.846749e+10  2.503619e+10   3431302085   \n",
       "2010/2/23       -387699136  2.422218e+10  2.360860e+10    613580581   \n",
       "2010/2/24        483080557  2.027639e+10  2.546846e+10  -5192069643   \n",
       "2010/2/25       -484640740  2.259238e+10  3.143820e+10  -8845826819   \n",
       "...                    ...           ...           ...          ...   \n",
       "2020/12/25       373419980  2.930742e+10  2.403325e+10   5274170949   \n",
       "2020/12/28       139024268  5.145881e+10  3.850894e+10  12949871214   \n",
       "2020/12/29      -767399344  4.903891e+10  5.029541e+10  -1256508512   \n",
       "2020/12/30      -683459786  7.361430e+10  4.678062e+10  26833686455   \n",
       "2020/12/31       366605219  5.869081e+10  5.299289e+10   5697920877   \n",
       "\n",
       "            FT_3components  FT_6components  FT_9components  FT_27components  \\\n",
       "Date                                                                          \n",
       "2010/2/10        96.358920       98.341083       94.157682        97.504515   \n",
       "2010/2/22        96.242195       98.002049       93.597325        95.929373   \n",
       "2010/2/23        96.125140       97.662731       93.037830        94.352361   \n",
       "2010/2/24        96.007757       97.323153       92.479275        92.775246   \n",
       "2010/2/25        95.890048       96.983341       91.921743        91.199799   \n",
       "...                    ...             ...             ...              ...   \n",
       "2020/12/25      111.049753      127.320992      121.886350       124.377775   \n",
       "2020/12/28      111.054398      127.489588      122.247031       124.754413   \n",
       "2020/12/29      111.058523      127.656402      122.607526       125.124296   \n",
       "2020/12/30      111.062129      127.821414      122.967770       125.487274   \n",
       "2020/12/31      111.065213      127.984605      123.327696       125.843220   \n",
       "\n",
       "            FT_81components  FT_100components  \n",
       "Date                                           \n",
       "2010/2/10         95.290389         94.816094  \n",
       "2010/2/22         90.414471         88.712660  \n",
       "2010/2/23         85.604138         82.741271  \n",
       "2010/2/24         80.907607         76.994296  \n",
       "2010/2/25         76.371194         71.557895  \n",
       "...                     ...               ...  \n",
       "2020/12/25       120.608542        121.100219  \n",
       "2020/12/28       121.300176        121.903124  \n",
       "2020/12/29       122.062133        122.757176  \n",
       "2020/12/30       122.888955        123.652424  \n",
       "2020/12/31       123.773486        124.578062  \n",
       "\n",
       "[2665 rows x 59 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', index_col = 'Date')\n",
    "data = data.iloc[:2665, :]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1664680815156,
     "user": {
      "displayName": "蕭仁鴻",
      "userId": "08849482633546520155"
     },
     "user_tz": -480
    },
    "id": "NKdzNEObeY1_",
    "outputId": "7e7a4254-0a92-4431-8617-fd8a81367d14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX: (2132, 59) trainY: (2132,)\n",
      "testX: (533, 59) testY: (533,)\n"
     ]
    }
   ],
   "source": [
    "data['y'] = data['Close']\n",
    "\n",
    "x = data.iloc[:, :59].values\n",
    "y = data.iloc[:, 59].values\n",
    "\n",
    "split = int(data.shape[0]* 0.8)\n",
    "train_x, test_x = x[: split, :], x[split:, :]\n",
    "train_y, test_y = y[: split, ], y[split: , ]\n",
    "\n",
    "print(f'trainX: {train_x.shape} trainY: {train_y.shape}')\n",
    "print(f'testX: {test_x.shape} testY: {test_y.shape}')\n",
    "\n",
    "x_scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "y_scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "train_x = x_scaler.fit_transform(train_x)\n",
    "test_x = x_scaler.transform(test_x)\n",
    "\n",
    "train_y = y_scaler.fit_transform(train_y.reshape(-1, 1))\n",
    "test_y = y_scaler.transform(test_y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1664680815157,
     "user": {
      "displayName": "蕭仁鴻",
      "userId": "08849482633546520155"
     },
     "user_tz": -480
    },
    "id": "N8p7ncnf4IfV"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, config, latent_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        modules = []\n",
    "        for i in range(1, len(config)):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(config[i - 1], config[i]),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(config[-1], latent_dim)\n",
    "        self.fc_var = nn.Linear(config[-1], latent_dim)\n",
    "\n",
    "        modules = []\n",
    "        self.decoder_input = nn.Linear(latent_dim, config[-1])\n",
    "\n",
    "        for i in range(len(config) - 1, 1, -1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(config[i], config[i - 1]),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )       \n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(config[1], config[0]),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        ) \n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "    def encode(self, x):\n",
    "        result = self.encoder(x)\n",
    "        mu = self.fc_mu(result)\n",
    "        logVar = self.fc_var(result)\n",
    "        return mu, logVar\n",
    "\n",
    "    def decode(self, x):\n",
    "        result = self.decoder(x)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu, logVar):\n",
    "        std = torch.exp(0.5* logVar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logVar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logVar)\n",
    "        output = self.decode(z)\n",
    "        return output, z, mu, logVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1664680815487,
     "user": {
      "displayName": "蕭仁鴻",
      "userId": "08849482633546520155"
     },
     "user_tz": -480
    },
    "id": "o86-npMa4YPF"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(train_x).float()), batch_size = 128, shuffle = False)\n",
    "model = VAE([59, 400, 400, 400, 10], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 42469,
     "status": "ok",
     "timestamp": 1664680860106,
     "user": {
      "displayName": "蕭仁鴻",
      "userId": "08849482633546520155"
     },
     "user_tz": -480
    },
    "id": "zG9GSE3Y4JiT",
    "outputId": "4c48a342-8643-4202-ba27-1af41f53c95b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/30] Loss: 757.5763816833496\n",
      "[2/30] Loss: 722.3392715454102\n",
      "[3/30] Loss: 706.48801612854\n",
      "[4/30] Loss: 713.1344528198242\n",
      "[5/30] Loss: 699.5037364959717\n",
      "[6/30] Loss: 676.3294658660889\n",
      "[7/30] Loss: 672.4543514251709\n",
      "[8/30] Loss: 676.6691837310791\n",
      "[9/30] Loss: 674.2499885559082\n",
      "[10/30] Loss: 662.9974021911621\n",
      "[11/30] Loss: 648.750846862793\n",
      "[12/30] Loss: 638.9697437286377\n",
      "[13/30] Loss: 636.9571914672852\n",
      "[14/30] Loss: 635.6328544616699\n",
      "[15/30] Loss: 626.1200885772705\n",
      "[16/30] Loss: 614.8800868988037\n",
      "[17/30] Loss: 609.9991474151611\n",
      "[18/30] Loss: 609.0666084289551\n",
      "[19/30] Loss: 608.4400863647461\n",
      "[20/30] Loss: 605.2052268981934\n",
      "[21/30] Loss: 597.861722946167\n",
      "[22/30] Loss: 586.1582698822021\n",
      "[23/30] Loss: 571.5990791320801\n",
      "[24/30] Loss: 558.2851524353027\n",
      "[25/30] Loss: 552.2269611358643\n",
      "[26/30] Loss: 554.8662395477295\n",
      "[27/30] Loss: 555.7859191894531\n",
      "[28/30] Loss: 544.2684707641602\n",
      "[29/30] Loss: 527.6774578094482\n",
      "[30/30] Loss: 517.752420425415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2585d8f5820>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() & use_cuda) else \"cpu\")\n",
    "num_epochs = 30 #originally 300\n",
    "learning_rate = 0.00003\n",
    "model = model.to(device)   \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "hist = np.zeros(num_epochs) \n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    loss_ = []\n",
    "    for (x, ) in train_loader:\n",
    "        x = x.to(device)\n",
    "        output, z, mu, logVar = model(x)\n",
    "        kl_divergence = 0.5* torch.sum(-1 - logVar + mu.pow(2) + logVar.exp())\n",
    "        loss = F.binary_cross_entropy(output, x) + kl_divergence\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_.append(loss.item())\n",
    "    hist[epoch] = sum(loss_)\n",
    "    print('[{}/{}] Loss:'.format(epoch+1, num_epochs), sum(loss_))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1664680860107,
     "user": {
      "displayName": "蕭仁鴻",
      "userId": "08849482633546520155"
     },
     "user_tz": -480
    },
    "id": "D7zNERpG5G75"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "_, VAE_train_x, train_x_mu, train_x_var = model(torch.from_numpy(train_x).float().to(device))\n",
    "_, VAE_test_x, test_x_mu, test_x_var = model(torch.from_numpy(test_x).float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1664680860108,
     "user": {
      "displayName": "蕭仁鴻",
      "userId": "08849482633546520155"
     },
     "user_tz": -480
    },
    "id": "FH35FCgeecpp"
   },
   "outputs": [],
   "source": [
    "def sliding_window(x, y, window):\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "    y_gan = []\n",
    "    for i in range(window, x.shape[0]):\n",
    "        tmp_x = x[i - window: i, :]\n",
    "        tmp_y = y[i]\n",
    "        tmp_y_gan = y[i - window: i + 1]\n",
    "        x_.append(tmp_x)\n",
    "        y_.append(tmp_y)\n",
    "        y_gan.append(tmp_y_gan)\n",
    "    x_ = torch.from_numpy(np.array(x_)).float()\n",
    "    y_ = torch.from_numpy(np.array(y_)).float()\n",
    "    y_gan = torch.from_numpy(np.array(y_gan)).float()\n",
    "    return x_, y_, y_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1664680860566,
     "user": {
      "displayName": "蕭仁鴻",
      "userId": "08849482633546520155"
     },
     "user_tz": -480
    },
    "id": "CvnkO3Xw7L8j"
   },
   "outputs": [],
   "source": [
    "train_x = np.concatenate((train_x, VAE_train_x.cpu().detach().numpy()), axis = 1)\n",
    "test_x = np.concatenate((test_x, VAE_test_x.cpu().detach().numpy()), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1664680860567,
     "user": {
      "displayName": "蕭仁鴻",
      "userId": "08849482633546520155"
     },
     "user_tz": -480
    },
    "id": "szKuUheceeLq",
    "outputId": "b3d94352-b72c-4296-fbd9-1362069ef89a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: torch.Size([2129, 3, 69]) train_y: torch.Size([2129, 1]) train_y_gan: torch.Size([2129, 4, 1])\n",
      "test_x: torch.Size([530, 3, 69]) test_y: torch.Size([530, 1]) test_y_gan: torch.Size([530, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "train_x_slide, train_y_slide, train_y_gan = sliding_window(train_x, train_y, 3)\n",
    "test_x_slide, test_y_slide, test_y_gan = sliding_window(test_x, test_y, 3)\n",
    "print(f'train_x: {train_x_slide.shape} train_y: {train_y_slide.shape} train_y_gan: {train_y_gan.shape}')\n",
    "print(f'test_x: {test_x_slide.shape} test_y: {test_y_slide.shape} test_y_gan: {test_y_gan.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowRegressionLSTM(nn.Module):\n",
    "    def __init__(self, num_sensors, hidden_units):\n",
    "        super().__init__()\n",
    "        self.num_sensors = num_sensors  # this is the number of features\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = 1\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_sensors,\n",
    "            hidden_size=hidden_units,\n",
    "            batch_first=True,\n",
    "            num_layers=self.num_layers\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        \n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        out = self.linear(hn[0]).flatten()  # First dim of Hn is num_layers, which is set to 1 above.\n",
    "\n",
    "        return out\n",
    "    \n",
    "class QLSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_size, \n",
    "                hidden_size, \n",
    "                n_qubits=4,\n",
    "                n_qlayers=1,\n",
    "                n_vrotations=3,\n",
    "                batch_first=True,\n",
    "                return_sequences=False, \n",
    "                return_state=False,\n",
    "                backend=\"lightning.qubit\"):\n",
    "        super(QLSTM, self).__init__()\n",
    "        self.n_inputs = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concat_size = self.n_inputs + self.hidden_size\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_qlayers = n_qlayers\n",
    "        self.n_vrotations = n_vrotations\n",
    "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.return_sequences = return_sequences\n",
    "        self.return_state = return_state\n",
    "        \n",
    "        self.wires_forget = [f\"wire_forget_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_input = [f\"wire_input_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_output = [f\"wire_output_{i}\" for i in range(self.n_qubits)]\n",
    "\n",
    "        self.dev_forget = qml.device(self.backend, wires=self.wires_forget)\n",
    "        self.dev_input = qml.device(self.backend, wires=self.wires_input)\n",
    "        self.dev_update = qml.device(self.backend, wires=self.wires_update)\n",
    "        self.dev_output = qml.device(self.backend, wires=self.wires_output)\n",
    "\n",
    "        #self.dev_forget = qml.device(self.backend, wires=self.n_qubits)\n",
    "        #self.dev_input = qml.device(self.backend, wires=self.n_qubits)\n",
    "        #self.dev_update = qml.device(self.backend, wires=self.n_qubits)\n",
    "        #self.dev_output = qml.device(self.backend, wires=self.n_qubits)\n",
    "        \n",
    "        def ansatz(params, wires_type):\n",
    "            # Entangling layer.\n",
    "            for i in range(1,3): \n",
    "                for j in range(self.n_qubits):\n",
    "                    if j + i < self.n_qubits:\n",
    "                        qml.CNOT(wires=[wires_type[j], wires_type[j + i]])\n",
    "                    else:\n",
    "                        qml.CNOT(wires=[wires_type[j], wires_type[j + i - self.n_qubits]])\n",
    "\n",
    "            # Variational layer.\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RX(params[0][i], wires=wires_type[i])\n",
    "                qml.RY(params[1][i], wires=wires_type[i])\n",
    "                qml.RZ(params[2][i], wires=wires_type[i])\n",
    "                \n",
    "        def VQC(features, weights, wires_type):\n",
    "            # Preproccess input data to encode the initial state.\n",
    "            #qml.templates.AngleEmbedding(features, wires=wires_type)\n",
    "            ry_params = [torch.arctan(feature) for feature in features]\n",
    "            rz_params = [torch.arctan(feature**2) for feature in features]\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.Hadamard(wires=wires_type[i])\n",
    "                qml.RY(ry_params[i], wires=wires_type[i])\n",
    "                qml.RZ(ry_params[i], wires=wires_type[i])\n",
    "        \n",
    "            #Variational block.\n",
    "            qml.layer(ansatz, self.n_qlayers, weights, wires_type = wires_type)\n",
    "\n",
    "        def _circuit_forget(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_forget)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_forget]\n",
    "        self.qlayer_forget = qml.QNode(_circuit_forget, self.dev_forget, interface=\"torch\")\n",
    "\n",
    "        def _circuit_input(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_input)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_input]\n",
    "        self.qlayer_input = qml.QNode(_circuit_input, self.dev_input, interface=\"torch\")\n",
    "\n",
    "        def _circuit_update(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_update)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_update]\n",
    "        self.qlayer_update = qml.QNode(_circuit_update, self.dev_update, interface=\"torch\")\n",
    "\n",
    "        def _circuit_output(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_output)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_output]\n",
    "        self.qlayer_output = qml.QNode(_circuit_output, self.dev_output, interface=\"torch\")\n",
    "\n",
    "        weight_shapes = {\"weights\": (self.n_qlayers, self.n_vrotations, self.n_qubits)}\n",
    "        print(f\"weight_shapes = (n_qlayers, n_vrotations, n_qubits) = ({self.n_qlayers}, {self.n_vrotations}, {self.n_qubits})\")\n",
    "\n",
    "        #self.clayer_in = torch.nn.Linear(self.concat_size, self.n_qubits)\n",
    "        self.clayer_in = torch.nn.Linear(128, 70)\n",
    "        self.VQC = {\n",
    "            'forget': qml.qnn.TorchLayer(self.qlayer_forget, weight_shapes),\n",
    "            'input': qml.qnn.TorchLayer(self.qlayer_input, weight_shapes),\n",
    "            'update': qml.qnn.TorchLayer(self.qlayer_update, weight_shapes),\n",
    "            'output': qml.qnn.TorchLayer(self.qlayer_output, weight_shapes)\n",
    "        }\n",
    "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
    "        #self.clayer_out = [torch.nn.Linear(n_qubits, self.hidden_size) for _ in range(4)]\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        '''\n",
    "        x.shape is (batch_size, seq_length, feature_size)\n",
    "        recurrent_activation -> sigmoid\n",
    "        activation -> tanh\n",
    "        '''\n",
    "        if self.batch_first is True:\n",
    "            batch_size, seq_length, features_size = x.size()\n",
    "        else:\n",
    "            seq_length, batch_size, features_size = x.size()\n",
    "\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size)  # hidden state (output)\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size)  # cell state\n",
    "        else:\n",
    "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\n",
    "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\n",
    "            h_t, c_t = init_states\n",
    "            h_t = h_t[0]\n",
    "            c_t = c_t[0]\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            # get features from the t-th element in seq, for all entries in the batch\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            # Concatenate input and hidden state\n",
    "            v_t = torch.cat((h_t, x_t), dim=1)\n",
    "\n",
    "            # match qubit dimension\n",
    "            y_t = self.clayer_in(v_t)\n",
    "\n",
    "            f_t = torch.sigmoid(self.clayer_out(self.VQC['forget'](y_t)))  # forget block\n",
    "            i_t = torch.sigmoid(self.clayer_out(self.VQC['input'](y_t)))  # input block\n",
    "            g_t = torch.tanh(self.clayer_out(self.VQC['update'](y_t)))  # update block\n",
    "            o_t = torch.sigmoid(self.clayer_out(self.VQC['output'](y_t))) # output block\n",
    "\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)\n",
    "    \n",
    "class QShallowRegressionLSTM(nn.Module):\n",
    "    def __init__(self, num_sensors, hidden_units, n_qubits=0, n_qlayers=1):\n",
    "        super().__init__()\n",
    "        self.num_sensors = num_sensors  # this is the number of features\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = 1\n",
    "\n",
    "        #self.lstm = nn.LSTM(\n",
    "        #    input_size=num_sensors,\n",
    "        #    hidden_size=hidden_units,\n",
    "        #    batch_first=True,\n",
    "        #    num_layers=self.num_layers\n",
    "        #)\n",
    "        \n",
    "        self.lstm = QLSTM(\n",
    "            input_size=num_sensors,\n",
    "            hidden_size=hidden_units,\n",
    "            batch_first=True,\n",
    "            n_qubits = n_qubits,\n",
    "            n_qlayers= n_qlayers\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        \n",
    "        # _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        _, (hn, _) = self.lstm(x, (h0, h0))\n",
    "        out = self.linear(hn).flatten()  # First dim of Hn is num_layers, which is set to 1 above.\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1664680860569,
     "user": {
      "displayName": "蕭仁鴻",
      "userId": "08849482633546520155"
     },
     "user_tz": -480
    },
    "id": "kWOh7BvDee2E"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.gru_1 = nn.GRU(input_size, 1024, batch_first = True)\n",
    "        self.gru_2 = nn.GRU(1024, 512, batch_first = True)\n",
    "        self.gru_3 = nn.GRU(512, 256, batch_first = True)\n",
    "        self.linear_1 = nn.Linear(256, 128)\n",
    "        self.linear_2 = nn.Linear(128, 64)\n",
    "        self.linear_3 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        use_cuda = 1\n",
    "        device = torch.device(\"cuda\" if (torch.cuda.is_available() & use_cuda) else \"cpu\")\n",
    "        h0 = torch.zeros(1, x.size(0), 1024).to(device)\n",
    "        out_1, _ = self.gru_1(x, h0)\n",
    "        out_1 = self.dropout(out_1)\n",
    "        h1 = torch.zeros(1, x.size(0), 512).to(device)\n",
    "        out_2, _ = self.gru_2(out_1, h1)\n",
    "        out_2 = self.dropout(out_2)\n",
    "        h2 = torch.zeros(1, x.size(0), 256).to(device)\n",
    "        out_3, _ = self.gru_3(out_2, h2)\n",
    "        out_3 = self.dropout(out_3)\n",
    "        out_4 = self.linear_1(out_3[:, -1, :])\n",
    "        out_5 = self.linear_2(out_4)\n",
    "        out = self.linear_3(out_5)\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(4, 32, kernel_size = 3, stride = 1, padding = 'same')\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size = 3, stride = 1, padding = 'same')\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size = 3, stride = 1, padding = 'same')\n",
    "        self.linear1 = nn.Linear(128, 220)\n",
    "        self.batch1 = nn.BatchNorm1d(220)\n",
    "        self.linear2 = nn.Linear(220, 220)\n",
    "        self.batch2 = nn.BatchNorm1d(220)\n",
    "        self.linear3 = nn.Linear(220, 1)\n",
    "        self.leaky = nn.LeakyReLU(0.01)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv1 = self.leaky(conv1)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv2 = self.leaky(conv2)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv3 = self.leaky(conv3)\n",
    "        flatten_x = conv3.reshape(conv3.shape[0], conv3.shape[1])\n",
    "        out_1 = self.linear1(flatten_x)\n",
    "        out_1 = self.leaky(out_1)\n",
    "        out_2 = self.linear2(out_1)\n",
    "        out_2 = self.relu(out_2)\n",
    "        out_3 = self.linear3(out_2)\n",
    "        out = self.sigmoid(out_3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51630,
     "status": "ok",
     "timestamp": 1664681381477,
     "user": {
      "displayName": "蕭仁鴻",
      "userId": "08849482633546520155"
     },
     "user_tz": -480
    },
    "id": "eQqyOLpPftiW",
    "outputId": "d4095aea-a7ab-4fef-b151-5e3031c72982",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_shapes = (n_qlayers, n_vrotations, n_qubits) = (1, 3, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AadiT\\anaconda3\\lib\\site-packages\\pennylane_lightning\\lightning_qubit.py:893: UserWarning: Pre-compiled binaries for lightning.qubit are not available. Falling back to using the Python-based default.qubit implementation. To manually compile from source, follow the instructions at https://pennylane-lightning.readthedocs.io/en/latest/installation.html.\n",
      "  warn(\n",
      "C:\\Users\\AadiT\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x70 and 128x70)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26088\\3579131464.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mfake_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mfake_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26088\\3108647297.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;31m# _, (hn, _) = self.lstm(x, (h0, c0))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# First dim of Hn is num_layers, which is set to 1 above.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26088\\3108647297.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, init_states)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;31m# match qubit dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[0my_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclayer_in\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mf_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclayer_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVQC\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'forget'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# forget block\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x70 and 128x70)"
     ]
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() & use_cuda) else \"cpu\")\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.00016\n",
    "num_epochs = 16 #originally 165\n",
    "\n",
    "trainDataloader = DataLoader(TensorDataset(train_x_slide, train_y_gan), batch_size = batch_size, shuffle = False)\n",
    "\n",
    "#modelG = Generator(59).to(device)\n",
    "modelG = QShallowRegressionLSTM(num_sensors=59, hidden_units=1).to(device)\n",
    "modelD = Discriminator().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizerG = torch.optim.Adam(modelG.parameters(), lr = learning_rate, betas = (0.0, 0.9))\n",
    "optimizerD = torch.optim.Adam(modelD.parameters(), lr = learning_rate, betas = (0.0, 0.9))\n",
    "\n",
    "histG = np.zeros(num_epochs)\n",
    "histD = np.zeros(num_epochs)\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    loss_G = []\n",
    "    loss_D = []\n",
    "    for (x, y) in trainDataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        fake_data = modelG(x)\n",
    "        fake_data = torch.cat([y[:, :3, :], fake_data.reshape(-1, 1, 1)], axis = 1)\n",
    "\n",
    "        dis_real_output = modelD(y)\n",
    "        real_labels = torch.ones_like(dis_real_output).to(device)\n",
    "        lossD_real = criterion(dis_real_output, real_labels)\n",
    "\n",
    "\n",
    "        dis_fake_output = modelD(fake_data)\n",
    "        fake_labels = torch.zeros_like(real_labels).to(device)\n",
    "        lossD_fake = criterion(dis_fake_output, fake_labels)\n",
    "\n",
    "        lossD = (lossD_real + lossD_fake)\n",
    "\n",
    "        modelD.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        optimizerD.step()\n",
    "        loss_D.append(lossD.item())\n",
    "\n",
    "        output_fake = modelD(fake_data)\n",
    "        lossG = criterion(output_fake, real_labels)\n",
    "        modelG.zero_grad()\n",
    "        lossG.backward()\n",
    "        optimizerG.step()\n",
    "        loss_G.append(lossG.item()) \n",
    "    histG[epoch] = sum(loss_G) \n",
    "    histD[epoch] = sum(loss_D)    \n",
    "    print(f'[{epoch+1}/{num_epochs}] LossD: {sum(loss_D)} LossG:{sum(loss_G)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1664681435405,
     "user": {
      "displayName": "蕭仁鴻",
      "userId": "08849482633546520155"
     },
     "user_tz": -480
    },
    "id": "BzXxXi5C2uDn",
    "outputId": "6da1a476-2df0-45ec-d729-444230b0fdd2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 6))\n",
    "plt.plot(histG, color = 'yellow', label = 'Generator Loss')\n",
    "plt.plot(histD, color = 'black', label = 'Discriminator Loss')\n",
    "plt.title('GAN Loss')\n",
    "plt.xlabel('Days')\n",
    "plt.legend(loc = 'upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1664681439276,
     "user": {
      "displayName": "蕭仁鴻",
      "userId": "08849482633546520155"
     },
     "user_tz": -480
    },
    "id": "G27wWqPygSz7"
   },
   "outputs": [],
   "source": [
    "modelG.eval()\n",
    "pred_y_train = modelG(train_x_slide.to(device))\n",
    "pred_y_test = modelG(test_x_slide.to(device))\n",
    "\n",
    "y_train_true = y_scaler.inverse_transform(train_y_slide)\n",
    "y_train_pred = y_scaler.inverse_transform(pred_y_train.cpu().detach().numpy())\n",
    "\n",
    "y_test_true = y_scaler.inverse_transform(test_y_slide)\n",
    "y_test_pred = y_scaler.inverse_transform(pred_y_test.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1664681441349,
     "user": {
      "displayName": "蕭仁鴻",
      "userId": "08849482633546520155"
     },
     "user_tz": -480
    },
    "id": "gw4Xb1D8gVG1",
    "outputId": "a0fc1486-4ea4-49a5-c63c-b6342978b1dc"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(y_train_true, color = 'blue', label = 'Acutal Price')\n",
    "plt.plot(y_train_pred, color = 'green', label = 'Predict Price')\n",
    "plt.title('GAN prediction training dataset')\n",
    "plt.ylabel('TWD')\n",
    "plt.xlabel('Days')\n",
    "plt.legend(loc = 'upper right')\n",
    "\n",
    "MSE = mean_squared_error(y_train_true, y_train_pred)\n",
    "RMSE = math.sqrt(MSE)\n",
    "print(f'Training dataset RMSE:{RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1664681445171,
     "user": {
      "displayName": "蕭仁鴻",
      "userId": "08849482633546520155"
     },
     "user_tz": -480
    },
    "id": "kqkiw48xgXA2",
    "outputId": "feeb7e6e-e848-404a-f8b8-aaf86282a771"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(y_test_true, color = 'blue', label = 'Acutal Price')\n",
    "plt.plot(y_test_pred, color = 'green', label = 'Predict Price')\n",
    "plt.title('GAN prediction testing dataset')\n",
    "plt.ylabel('TWD')\n",
    "plt.xlabel('Days')\n",
    "plt.legend(loc = 'upper right')\n",
    "\n",
    "MSE = mean_squared_error(y_test_true, y_test_pred)\n",
    "RMSE = math.sqrt(MSE)\n",
    "print(f'Training dataset RMSE:{RMSE}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOUwN7lQIP6ZjvMgVaOuPXg",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
