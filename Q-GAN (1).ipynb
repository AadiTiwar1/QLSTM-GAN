{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ccd075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import pennylane as qml\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, target, features, sequence_length=5):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.y = torch.tensor(dataframe[self.target].values).float()\n",
    "        self.X = torch.tensor(dataframe[self.features].values).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i): \n",
    "        if i >= self.sequence_length - 1:\n",
    "            i_start = i - self.sequence_length + 1\n",
    "            x = self.X[i_start:(i + 1), :]\n",
    "        else:\n",
    "            padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n",
    "            x = self.X[0:(i + 1), :]\n",
    "            x = torch.cat((padding, x), 0)\n",
    "\n",
    "        return x, self.y[i]\n",
    "\n",
    "\n",
    "class QLSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_size, \n",
    "                hidden_size, \n",
    "                n_qubits=4,\n",
    "                n_qlayers=1,\n",
    "                n_vrotations=3,\n",
    "                batch_first=True,\n",
    "                return_sequences=False, \n",
    "                return_state=False,\n",
    "                backend=\"default.qubit\"):\n",
    "        super(QLSTM, self).__init__()\n",
    "        self.n_inputs = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concat_size = self.n_inputs + self.hidden_size\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_qlayers = n_qlayers\n",
    "        self.n_vrotations = n_vrotations\n",
    "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.return_sequences = return_sequences\n",
    "        self.return_state = return_state\n",
    "        \n",
    "        self.wires_forget = [f\"wire_forget_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_input = [f\"wire_input_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_output = [f\"wire_output_{i}\" for i in range(self.n_qubits)]\n",
    "\n",
    "        self.dev_forget = qml.device(self.backend, wires=self.wires_forget)\n",
    "        self.dev_input = qml.device(self.backend, wires=self.wires_input)\n",
    "        self.dev_update = qml.device(self.backend, wires=self.wires_update)\n",
    "        self.dev_output = qml.device(self.backend, wires=self.wires_output)\n",
    "\n",
    "        #self.dev_forget = qml.device(self.backend, wires=self.n_qubits)\n",
    "        #self.dev_input = qml.device(self.backend, wires=self.n_qubits)\n",
    "        #self.dev_update = qml.device(self.backend, wires=self.n_qubits)\n",
    "        #self.dev_output = qml.device(self.backend, wires=self.n_qubits)\n",
    "        \n",
    "        def ansatz(params, wires_type):\n",
    "            # Entangling layer.\n",
    "            for i in range(1,3): \n",
    "                for j in range(self.n_qubits):\n",
    "                    if j + i < self.n_qubits:\n",
    "                        qml.CNOT(wires=[wires_type[j], wires_type[j + i]])\n",
    "                    else:\n",
    "                        qml.CNOT(wires=[wires_type[j], wires_type[j + i - self.n_qubits]])\n",
    "\n",
    "            # Variational layer.\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RX(params[0][i], wires=wires_type[i])\n",
    "                qml.RY(params[1][i], wires=wires_type[i])\n",
    "                qml.RZ(params[2][i], wires=wires_type[i])\n",
    "                \n",
    "        def VQC(features, weights, wires_type):\n",
    "            # Preproccess input data to encode the initial state.\n",
    "            #qml.templates.AngleEmbedding(features, wires=wires_type)\n",
    "            ry_params = [torch.arctan(feature) for feature in features]\n",
    "            rz_params = [torch.arctan(feature**2) for feature in features]\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.Hadamard(wires=wires_type[i])\n",
    "                qml.RY(ry_params[i], wires=wires_type[i])\n",
    "                qml.RZ(ry_params[i], wires=wires_type[i])\n",
    "        \n",
    "            #Variational block.\n",
    "            qml.layer(ansatz, self.n_qlayers, weights, wires_type = wires_type)\n",
    "\n",
    "        def _circuit_forget(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_forget)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_forget]\n",
    "        self.qlayer_forget = qml.QNode(_circuit_forget, self.dev_forget, interface=\"torch\")\n",
    "\n",
    "        def _circuit_input(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_input)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_input]\n",
    "        self.qlayer_input = qml.QNode(_circuit_input, self.dev_input, interface=\"torch\")\n",
    "\n",
    "        def _circuit_update(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_update)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_update]\n",
    "        self.qlayer_update = qml.QNode(_circuit_update, self.dev_update, interface=\"torch\")\n",
    "\n",
    "        def _circuit_output(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_output)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_output]\n",
    "        self.qlayer_output = qml.QNode(_circuit_output, self.dev_output, interface=\"torch\")\n",
    "\n",
    "        weight_shapes = {\"weights\": (self.n_qlayers, self.n_vrotations, self.n_qubits)}\n",
    "        print(f\"weight_shapes = (n_qlayers, n_vrotations, n_qubits) = ({self.n_qlayers}, {self.n_vrotations}, {self.n_qubits})\")\n",
    "\n",
    "        self.clayer_in = torch.nn.Linear(self.concat_size, self.n_qubits)\n",
    "        self.VQC = {\n",
    "            'forget': qml.qnn.TorchLayer(self.qlayer_forget, weight_shapes),\n",
    "            'input': qml.qnn.TorchLayer(self.qlayer_input, weight_shapes),\n",
    "            'update': qml.qnn.TorchLayer(self.qlayer_update, weight_shapes),\n",
    "            'output': qml.qnn.TorchLayer(self.qlayer_output, weight_shapes)\n",
    "        }\n",
    "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
    "        #self.clayer_out = [torch.nn.Linear(n_qubits, self.hidden_size) for _ in range(4)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x.shape is (batch_size, seq_length, feature_size)\n",
    "        recurrent_activation -> sigmoid\n",
    "        activation -> tanh\n",
    "        '''\n",
    "        if self.batch_first is True:\n",
    "            batch_size, seq_length, features_size = x.size()\n",
    "        else:\n",
    "            seq_length, batch_size, features_size = x.size()\n",
    "\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size)  # hidden state (output)\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size)  # cell state\n",
    "        else:\n",
    "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\n",
    "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\n",
    "            h_t, c_t = init_states\n",
    "            h_t = h_t[0]\n",
    "            c_t = c_t[0]\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            # get features from the t-th element in seq, for all entries in the batch\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            # Concatenate input and hidden state\n",
    "            v_t = torch.cat((h_t, x_t), dim=1)\n",
    "\n",
    "            # match qubit dimension\n",
    "            y_t = self.clayer_in(v_t)\n",
    "\n",
    "            f_t = torch.sigmoid(self.clayer_out(self.VQC['forget'](y_t)))  # forget block\n",
    "            i_t = torch.sigmoid(self.clayer_out(self.VQC['input'](y_t)))  # input block\n",
    "            g_t = torch.tanh(self.clayer_out(self.VQC['update'](y_t)))  # update block\n",
    "            o_t = torch.sigmoid(self.clayer_out(self.VQC['output'](y_t))) # output block\n",
    "\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)\n",
    "    \n",
    "class QShallowRegressionLSTM(nn.Module):\n",
    "    def __init__(self, num_sensors, hidden_units, n_qubits=0, n_qlayers=1):\n",
    "        super().__init__()\n",
    "        self.num_sensors = num_sensors  # this is the number of features\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = 1\n",
    "\n",
    "        #self.lstm = nn.LSTM(\n",
    "        #    input_size=num_sensors,\n",
    "        #    hidden_size=hidden_units,\n",
    "        #    batch_first=True,\n",
    "        #    num_layers=self.num_layers\n",
    "        #)\n",
    "        \n",
    "        self.lstm = QLSTM(\n",
    "            input_size=num_sensors,\n",
    "            hidden_size=hidden_units,\n",
    "            batch_first=True,\n",
    "            n_qubits = n_qubits,\n",
    "            n_qlayers= n_qlayers\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        \n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        out = self.linear(hn).flatten()  # First dim of Hn is num_layers, which is set to 1 above.\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2d0608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pickle import load\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from tensorflow.keras.layers import GRU, Dense, Flatten, Conv1D, BatchNormalization, LeakyReLU, ELU, ReLU\n",
    "from tensorflow.keras import Sequential, regularizers\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# Load data\n",
    "X_train = np.load(\"X_train.npy\", allow_pickle=True)\n",
    "y_train = np.load(\"y_train.npy\", allow_pickle=True)\n",
    "X_test = np.load(\"X_test.npy\", allow_pickle=True)\n",
    "y_test = np.load(\"y_test.npy\", allow_pickle=True)\n",
    "yc_train = np.load(\"yc_train.npy\", allow_pickle=True)\n",
    "yc_test = np.load(\"yc_test.npy\", allow_pickle=True)\n",
    "\n",
    "#Define the generator\n",
    "def make_generator_model(input_dim, output_dim, feature_size) -> tf.keras.models.Model:\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=1024, return_sequences = True, input_shape=(input_dim, feature_size),\n",
    "                  recurrent_dropout=0.2))\n",
    "    model.add(GRU(units=512, return_sequences = True, recurrent_dropout=0.2)) # 256, return_sequences = True\n",
    "    model.add(GRU(units=256, recurrent_dropout=0.2)) #, recurrent_dropout=0.1\n",
    "    # , recurrent_dropout = 0.2\n",
    "    model.add(Dense(128))\n",
    "    # model.add(Dense(128))\n",
    "    model.add(Dense(64))\n",
    "    #model.add(Dense(16))\n",
    "    model.add(Dense(units=output_dim))\n",
    "    return model\n",
    "\n",
    "# Define the discriminator\n",
    "def Discriminator() -> tf.keras.models.Model:\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv1D(32, input_shape=(4, 1), kernel_size=3, strides=2, padding=\"same\", activation=LeakyReLU(alpha=0.01)))\n",
    "    model.add(Conv1D(64, kernel_size=3, strides=2, padding=\"same\", activation=LeakyReLU(alpha=0.01)))\n",
    "    model.add(Conv1D(128, kernel_size=3, strides=2, padding=\"same\", activation=LeakyReLU(alpha=0.01)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(220, use_bias=True))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(220, use_bias=True))\n",
    "    model.add(ReLU())\n",
    "    model.add(Dense(1))\n",
    "    return model\n",
    "\n",
    "# Train WGAN-GP model\n",
    "class GAN():\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(GAN, self).__init__()\n",
    "        self.d_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "        self.g_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.batch_size = 128\n",
    "        \n",
    "    def gradient_penalty(self, batch_size, real_output, fake_output):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # get the interpolated data\n",
    "        alpha = tf.random.normal([batch_size, 4, 1], 0.0, 1.0)\n",
    "        diff = fake_output - tf.cast(real_output, tf.float32)\n",
    "        interpolated = tf.cast(real_output, tf.float32) + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "\n",
    "        # 3. Calcuate the norm of the gradients\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
    "\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, data):\n",
    "        real_input, real_price, yc = data\n",
    "        batch_size = tf.shape(real_input)[0]\n",
    "        for _ in range(1):\n",
    "            with tf.GradientTape() as d_tape:\n",
    "                # Train the discriminator\n",
    "                # generate fake output\n",
    "                \n",
    "                generated_data = self.generator(real_input, training=True)\n",
    "                # reshape the data\n",
    "                generated_data_reshape = tf.reshape(generated_data, [generated_data.shape[0], generated_data.shape[1], 1])\n",
    "                fake_output = tf.concat([generated_data_reshape, tf.cast(yc, tf.float32)], axis=1)\n",
    "                real_y_reshape = tf.reshape(real_price, [real_price.shape[0], real_price.shape[1], 1])\n",
    "                real_output = tf.concat([tf.cast(real_y_reshape, tf.float32), tf.cast(yc, tf.float32)], axis=1)\n",
    "                # Get the logits for the fake images\n",
    "                \n",
    "                D_real = self.discriminator(real_output, training=True)\n",
    "                \n",
    "                # Get the logits for real images\n",
    "                \n",
    "                D_fake = self.discriminator(fake_output, training=True)\n",
    "                \n",
    "                # Calculate discriminator loss using fake and real logits\n",
    "                real_loss = tf.cast(tf.reduce_mean(D_real), tf.float32)\n",
    "                fake_loss = tf.cast(tf.reduce_mean(D_fake), tf.float32)\n",
    "                d_cost = fake_loss-real_loss\n",
    "                # Calculate the gradientjiu penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_output, fake_output)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * 10\n",
    "\n",
    "            d_grads = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            self.d_optimizer.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))\n",
    "        for _ in range(3):\n",
    "            with tf.GradientTape() as g_tape:\n",
    "                # Train the generator\n",
    "                # generate fake output\n",
    "                generated_data = self.generator(real_input,  training=True)\n",
    "                # reshape the data\n",
    "                generated_data_reshape = tf.reshape(generated_data, [generated_data.shape[0], generated_data.shape[1], 1])\n",
    "                fake_output = tf.concat([generated_data_reshape, tf.cast(yc, tf.float32)], axis=1)\n",
    "                # Get the discriminator logits for fake images\n",
    "                G_fake = self.discriminator(fake_output, training=True)\n",
    "                # Calculate the generator loss\n",
    "                g_loss = -tf.reduce_mean(G_fake)\n",
    "            g_grads = g_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "            self.g_optimizer.apply_gradients(zip(g_grads, self.generator.trainable_variables))\n",
    "\n",
    "        return real_price, generated_data, {'d_loss': d_loss, 'g_loss': g_loss}\n",
    "\n",
    "    def train(self, X_train, y_train, yc, epochs):\n",
    "        data = X_train, y_train, yc\n",
    "        train_hist = {}\n",
    "        train_hist['D_losses'] = []\n",
    "        train_hist['G_losses'] = []\n",
    "        train_hist['per_epoch_times'] = []\n",
    "        train_hist['total_ptime'] = []\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "\n",
    "            real_price, fake_price, loss = self.train_step(data)\n",
    "\n",
    "            G_losses = []\n",
    "            D_losses = []\n",
    "\n",
    "            Real_price = []\n",
    "            Predicted_price = []\n",
    "\n",
    "            D_losses.append(loss['d_loss'].numpy())\n",
    "            G_losses.append(loss['g_loss'].numpy())\n",
    "\n",
    "            Predicted_price.append(fake_price)\n",
    "            Real_price.append(real_price)\n",
    "\n",
    "            # Save the model every 15 epochs\n",
    "            if (epoch + 1) % 15 == 0:\n",
    "                tf.keras.models.save_model(generator, 'gen_GRU_model_%d.h5' % epoch)\n",
    "                self.checkpoint.save(file_prefix=self.checkpoint_prefix)\n",
    "                print('epoch', epoch+1, 'd_loss', loss['d_loss'].numpy(), 'g_loss', loss['g_loss'].numpy())\n",
    "\n",
    "            # For printing loss\n",
    "            epoch_end_time = time.time()\n",
    "            per_epoch_ptime = epoch_end_time - start\n",
    "            train_hist['D_losses'].append(D_losses)\n",
    "            train_hist['G_losses'].append(G_losses)\n",
    "            train_hist['per_epoch_times'].append(per_epoch_ptime)\n",
    "            \n",
    "        # Reshape the predicted result & real\n",
    "        Predicted_price = np.array(Predicted_price)\n",
    "        Predicted_price = Predicted_price.reshape(Predicted_price.shape[1], Predicted_price.shape[2])\n",
    "        Real_price = np.array(Real_price)\n",
    "        Real_price = Real_price.reshape(Real_price.shape[1], Real_price.shape[2])\n",
    "\n",
    "        # Plot the loss\n",
    "        plt.plot(train_hist['D_losses'], label='D_loss')\n",
    "        plt.plot(train_hist['G_losses'], label='G_loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.savefig('train_loss.png')\n",
    "\n",
    "        print(\"REAL\", Real_price.shape)\n",
    "        print(Real_price)\n",
    "        print(\"PREDICTED\", Predicted_price.shape)\n",
    "        print(Predicted_price)\n",
    "\n",
    "        return Predicted_price, Real_price, np.sqrt(mean_squared_error(Real_price, Predicted_price)) / np.mean(Real_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "921ab053",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_shapes = (n_qlayers, n_vrotations, n_qubits) = (1, 3, 4)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'training'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_129496\\3163745949.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdiscriminator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mgan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mPredicted_price\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReal_price\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRMSPE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myc_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_129496\\308628345.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X_train, y_train, yc, epochs)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             \u001b[0mreal_price\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_price\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0mG_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_129496\\308628345.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[1;31m# Train the discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[1;31m# generate fake output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                 \u001b[0mgenerated_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m36\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m                 \u001b[1;31m# reshape the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                 \u001b[0mgenerated_data_reshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgenerated_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerated_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'training'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    input_dim = X_train.shape[1]\n",
    "    feature_size = X_train.shape[2]\n",
    "    output_dim = y_train.shape[1]\n",
    "    epoch = 1\n",
    "\n",
    "    # generator = Generator(X_train.shape[1], output_dim, X_train.shape[2])\n",
    "    # generator = QShallowRegressionLSTM(num_sensors=feature_size, hidden_units=4, n_qubits=4)\n",
    "    generator = QLSTM(input_size=input_dim, hidden_size=16)\n",
    "    discriminator = Discriminator()\n",
    "    gan = GAN(generator, discriminator)\n",
    "    Predicted_price, Real_price, RMSPE = gan.train(X_train, y_train, yc_train, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e308bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
